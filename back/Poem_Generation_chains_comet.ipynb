{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uc5xkvlr7GVoGJR9jSZo1UdI-c2M-Vr3","timestamp":1721563202299}],"authorship_tag":"ABX9TyNrAEZ08CN7jKFJknQ9CV5h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Installations de bibliothèques"],"metadata":{"id":"riGIekFIq4Vc"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"aMLUIrQqAE31","executionInfo":{"status":"ok","timestamp":1722419363837,"user_tz":-120,"elapsed":33469,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd08d116-2133-4ed3-eda6-30f4f01ed407"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.3/682.3 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.6/866.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.6/303.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install transformers langchain_google_genai mistralai langchain-mistralai langchain_core langchain langchain_community langchain_experimental langchainhub langchain-anthropic comet-ml comet-llm rapidfuzz -Uq"]},{"cell_type":"markdown","source":["# Imports necéssaires au projet"],"metadata":{"id":"0wWLiRviq9O-"}},{"cell_type":"code","source":["import os\n","import uuid\n","import json\n","import time\n","\n","# Google Generative AI\n","from langchain_google_genai import GoogleGenerativeAI, ChatGoogleGenerativeAI\n","\n","# Mistral AI\n","from mistralai.client import MistralClient\n","from mistralai.models.chat_completion import ChatMessage\n","from langchain_mistralai.chat_models import ChatMistralAI\n","\n","# Comet\n","import comet_ml, comet_llm\n","from comet_ml import Experiment\n","from langchain_community.callbacks.tracers.comet import CometTracer\n","\n","# HuggingFace\n","from transformers import pipeline, AutoTokenizer, T5ForConditionalGeneration\n","\n","# Langchain\n","from langchain import HuggingFaceHub\n","from langchain.chains import LLMChain, SequentialChain\n","from langchain.memory import ConversationBufferMemory\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.agents import AgentExecutor, create_react_agent\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, AIMessagePromptTemplate"],"metadata":{"id":"Ea_ydAUZb7hi","executionInfo":{"status":"ok","timestamp":1722419571534,"user_tz":-120,"elapsed":264,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Variables d'environnement"],"metadata":{"id":"XM-xQ_R0rDev"}},{"cell_type":"code","source":["os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n","\n","os.environ['GOOGLE_CSE_ID'] = 'c118258383fab4969'\n","os.environ['GOOGLE_API_KEY'] = \"AIzaSyDI4gpwnwFsta6WkVsnRrcJxzZzgHHSunE\"\n","\n","os.environ[\"MISTRAL_API_KEY\"] = \"B2LwD2FxKdb9g9jsnV2jb6hQGiw1h2yV\"\n","\n","os.environ[\"COMET_API_KEY\"] = \"g9Um8JaLLAjkjVKYPZjYLXvcP\"\n","\n","os.environ[\"LANGCHAIN_API_KEY\"] =\"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\""],"metadata":{"id":"AsPj5NY_qgXi","executionInfo":{"status":"ok","timestamp":1722419571890,"user_tz":-120,"elapsed":2,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Connexion à Comet"],"metadata":{"id":"P5sXgRNSrHyR"}},{"cell_type":"code","source":["comet_llm.init(project=\"poem_generation_chains_comet\")"],"metadata":{"id":"YhaUk4hNq2i7","executionInfo":{"status":"ok","timestamp":1722419572841,"user_tz":-120,"elapsed":298,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Création des chaînes avec HuggingFace"],"metadata":{"id":"QDFxlxLebtEo"}},{"cell_type":"code","source":["# Définition du modèle\n","from transformers import pipeline, AutoTokenizer, T5ForConditionalGeneration\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n","hfm = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"pimwfIacbz7I","executionInfo":{"status":"ok","timestamp":1722419573748,"user_tz":-120,"elapsed":908,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Pemière méthode : appel simple"],"metadata":{"id":"xGkYCUG0hFgX"}},{"cell_type":"code","source":["query = \"Can you write a haiku about summer ?\"\n","response = hfm(query)\n","print(response)\n","comet_llm.log_prompt(prompt=query, output=response[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azvczYhigpRI","executionInfo":{"status":"ok","timestamp":1722419574813,"user_tz":-120,"elapsed":1067,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}},"outputId":"7b5a8cb9-8083-46f5-d03a-e0d9cc1e3474"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[{'generated_text': 'summer is a time of year .'}]\n"]},{"output_type":"execute_result","data":{"text/plain":["LLMResult(id='2fb80badcfb544bb9b61cd31199770d1', project_url='https://www.comet.com/emeline-caruana/poem_generation_chains_comet')"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Deuxième méthode : création de fonction avec tag"],"metadata":{"id":"1yGfMD4NhIxJ"}},{"cell_type":"code","source":["def poem_gen(query1, query2, tags, metadata):\n","    start = time.time()\n","    prompt_template = \"Write a poem in this form : {form}. The poem must be about this topic : {topic}.\"\n","    prompt = prompt_template.format(form=query1, topic=query2)\n","    output = hfm(prompt)\n","    end = time.time()\n","    duration = end - start\n","    comet_llm.log_prompt(prompt=prompt, output=output[0][\"generated_text\"], tags=tags, metadata=metadata, duration = duration*1000, prompt_template=prompt_template, prompt_template_variables=[\"form\",\"topic\"])\n","    return output\n","\n","tags=[\"poem\",\"T5\"]\n","metadata={\"dataset\":\"poem_dataset\", \"model\":\"google/flan-t5-small\", \"max_tokens\":50, \"temperature\":0.1}\n","\n","poem_gen(\"poem\", \"summer\", tags, metadata)\n","poem_gen(\"haiku\", \"summer\", tags, metadata)\n","poem_gen(\"ballad\", \"summer\",  tags, metadata)\n","poem_gen(\"poem\", \"friendship\",  tags, metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUUvAqSrgred","executionInfo":{"status":"ok","timestamp":1722419579700,"user_tz":-120,"elapsed":4892,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}},"outputId":"ede07d2e-f18f-4394-e68c-e1def8464dd7"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'friendship is the best friend i have ever had .'}]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# Création des chaînes avec Google Gemini"],"metadata":{"id":"5M4fQVRirR_e"}},{"cell_type":"code","source":["llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",temperature=0.1)\n","callback = CometTracer()\n","\n","prompt_template = \"Write a poem in this form : {form}. The poem must be about this topic : {topic}.\"\n","prompt = PromptTemplate(template=prompt_template, input_variables=[\"form\",\"topic\"], callbacks=[callback])\n","\n","chain = LLMChain(llm=llm, prompt=prompt)\n","\n","result = chain.predict(form=\"poem\", topic=\"summer\", callbacks=[callback])\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbZXoap5qrFQ","executionInfo":{"status":"ok","timestamp":1722419583926,"user_tz":-120,"elapsed":4241,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}},"outputId":"5e035e43-5b3d-4eaf-8b77-22c1ce8ecaf4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["**Summer's Embrace**\n","\n","Golden rays, a warm embrace,\n","Nature's symphony, a vibrant chase.\n","Birdsong fills the air with cheer,\n","As summer's magic draws near.\n","\n","Fields of green, a verdant hue,\n","Flowers bloom, a vibrant crew.\n","Butterflies dance on gentle breeze,\n","A symphony of colors, a summer's ease.\n","\n","Sun-kissed skin, a golden glow,\n","Laughter echoes, a joyful flow.\n","Picnics by the sparkling stream,\n","Summer's bliss, a blissful dream.\n","\n","Long days stretch into the night,\n","Stars twinkle, a celestial sight.\n","Fireflies flicker, a magical dance,\n","Guiding us through summer's trance.\n","\n","As summer's reign begins to wane,\n","Memories linger, a sweet refrain.\n","The warmth and joy, a cherished treasure,\n","Summer's embrace, a timeless pleasure.\n"]}]},{"cell_type":"code","source":["def poem_gen_gemini(query1, query2, tags, metadata):\n","    start = time.time()\n","    prompt_template = \"Write a poem in this form : {form}. The poem must be about this topic : {topic}.\"\n","    prompt = PromptTemplate(template=prompt_template, input_variables=[\"form\",\"topic\"])\n","    chain = LLMChain(llm=llm, prompt=prompt)\n","    output = chain.predict(form=query1, topic=query2)\n","\n","    prompt_dict = {\n","        \"template\": prompt.template,\n","        \"input_variables\": prompt.input_variables\n","    }\n","\n","    end = time.time()\n","    duration = end - start\n","    comet_llm.log_prompt(prompt=prompt_dict, output=output, tags=tags, metadata=metadata, duration = duration*1000, prompt_template=prompt_template, prompt_template_variables=[\"form\",\"topic\"])\n","    return output\n","\n","tags=[\"poem\",\"gemini\"]\n","metadata={\"dataset\":\"poem_dataset\", \"model\":\"gemini-pro\", \"max_tokens\":50, \"temperature\":0.1}\n","\n","poem_gen_gemini(\"poem\", \"summer\", tags, metadata)\n","poem_gen_gemini(\"haiku\", \"summer\", tags, metadata)\n","poem_gen_gemini(\"ballad\", \"summer\",  tags, metadata)\n","poem_gen_gemini(\"poem\", \"friendship\",  tags, metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"id":"-dV3EBPXoWNF","executionInfo":{"status":"ok","timestamp":1722419599525,"user_tz":-120,"elapsed":15614,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}},"outputId":"f0328d05-1ca4-4f29-d391-c4a1050879ab"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"**Ode to Friendship**\\n\\nIn life's tapestry, a vibrant hue,\\nFriendship's threads weave, a bond so true.\\nThrough laughter's echoes and tears' embrace,\\nIt stands as a beacon, a sacred space.\\n\\nLike a sturdy oak, it weathers storms,\\nIts roots entwined, its branches strong and warm.\\nThrough trials and triumphs, it remains,\\nA constant source of solace and gains.\\n\\nIn moments of doubt, it offers a light,\\nGuiding us through darkness, making things right.\\nIt whispers words of encouragement and cheer,\\nDispelling fears, banishing every tear.\\n\\nIn times of joy, it amplifies our bliss,\\nSharing laughter, making memories we can't miss.\\nIt celebrates our victories, great and small,\\nA chorus of voices, answering our call.\\n\\nFriendship's embrace is a precious gift,\\nA treasure to cherish, a bond that can lift.\\nIt enriches our lives, making them complete,\\nA tapestry woven with love and sweet.\\n\\nSo let us honor this bond so dear,\\nNurturing it with care, year after year.\\nFor in friendship's embrace, we find our way,\\nA beacon of love, guiding us each day.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# Création des chaînes avec Mistral AI"],"metadata":{"id":"cvUL3uhivxob"}},{"cell_type":"code","source":["mistral_client = MistralClient()"],"metadata":{"id":"y50d7J0wv0bI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat = mistral_client.chat(\n","    model = \"mistral-large-latest\",\n","    temperature = 0.5,\n","    messages=[ChatMessage(role=\"user\", content=\"Generate a poem about life\")]\n",")\n","print(chat.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"VskSpVBwJCxt","executionInfo":{"status":"error","timestamp":1721562862078,"user_tz":-120,"elapsed":796,"user":{"displayName":"Emeline Caruana","userId":"00002816483261657383"}},"outputId":"6f8d2203-3d0f-4f46-a1e2-815a4b393983"},"execution_count":null,"outputs":[{"output_type":"error","ename":"MistralAPIException","evalue":"Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMistralAPIException\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-a7dde53c6873>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m chat = mistral_client.chat(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistral-large-latest\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generate a poem about life\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mistralai/client.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, messages, model, tools, temperature, max_tokens, top_p, random_seed, safe_mode, safe_prompt, tool_choice, response_format)\u001b[0m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msingle_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatCompletionResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mistralai/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, json, path, stream, attempt, data, check_model_deprecation_headers_callback, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcheck_model_deprecation_headers_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mcheck_model_deprecation_headers_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mConnectError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mistralai/client.py\u001b[0m in \u001b[0;36m_check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_response_status_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mistralai/client.py\u001b[0m in \u001b[0;36m_check_response_status_codes\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             raise MistralAPIException.from_response(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Status: {response.status_code}. Message: {response.text}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMistralAPIException\u001b[0m: Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}"]}]},{"cell_type":"code","source":["chat = ChatMistralAI()\n","\n","prompt = ChatPromptTemplate.from_template(\"Generate a poem about this topic {topic}\")\n","parser = StrOutputParser()\n","chaine = prompt | chat | parser\n","reponse = chaine.invoke({\"topic\":\"life\"})\n","print(reponse)"],"metadata":{"id":"EoHOq2pwJHAp"},"execution_count":null,"outputs":[]}]}